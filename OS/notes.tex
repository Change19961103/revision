\documentclass[a4]{article}
\usepackage{parskip}
\usepackage[margin=8em]{geometry}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{float}
\usepackage{listings}

\graphicspath{{.}}

% Thanks to Angus Pearson for nagging me to fuck about fonts in
% XeLaTeX

\usepackage{fontspec} \usepackage{xltxtra}

% PT Serif
\setromanfont[ BoldFont=PTF75F.ttf, ItalicFont=PTZ56F.ttf,
BoldItalicFont=PTF76F.ttf, ]{PTF55F.ttf}

% Noto Sans
\setsansfont[ BoldFont=NotoSans-Bold.ttf,
ItalicFont=NotoSans-Italic.ttf, BoldItalicFont=NotoSans-BoldItalic.ttf
]{NotoSans-Regular.ttf}

\begin{document}

\textbf{\Huge Operating Systems Notes}

\tableofcontents

\section{OS Architecture}

Different architectures dictate the way that an operating system may conduct its business.

As different architecture platforms have different instruction sets and ASPLOS features, it determines what are viable methods for accomplishing tasks such as \textit{memory protection} and \textit{control interrupts}.

The OS acts as an intermediary between software and hardware. It allows mediation of access and gives developers a consistent API to access hardware and low-level operations. This abstraction is achieved via \textit{traps} and \textit{exceptions}. Similarly, devices can gain attention via the use of \textit{interrupts}.

An OS is intended to be a good balance between \textit{user needs} and \textit{system needs}.

A user wants an OS to be \textit{fast, reliable and safe}; an OS developer wants it to be \textit{efficient, error-free, and easy to maintain and implement}.

The design of an operating systems makes a distinction between two underlying principles:

\textbf{Policy}, which is \textit{what} will be done; \textbf{Mechanism}, which his \textit{how} to do it.

\subsection{Structure of an OS}

A capable operating system is home to a multitude of components: memory management, I/O, file systems, command interpreters and so on. There are multiple ways to link to structure how all these components are connected to each other.

\subsubsection{Monolithic}

An early type of design for an OS, where the \textit{entire} operating system would function in \textit{kernel mode} (see the \textit{Privileged Instructions} section). However, this design carried many issues with maintainability and reliability, and was hard to make sense of.

\subsubsection{Layering}

As opposed to monolithic, this method implemented the OS as a \textit{set of layers} instead, where a layer could present itself to the corresponding layer above.

\begin{figure}[H]
  \centering
  \begin{center}
    \begin{tabular}{|c|c|l|}
      \hline
      5 & \textbf{Job Managers} & Execute a user's programs\\
      \hline
      4 & \textbf{Device Managers} & Handle devices, provide buffering\\
      \hline
      3 & \textbf{Console Manager} & Provide virtual consoles\\
      \hline
      2 & \textbf{Page Manager} & Implement virtual memories for each process\\
      \hline
      1 & \textbf{Kernel} & Implement a virtual processor for each process\\
      \hline
      0 & \textbf{Hardware} & \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Dijkstra's THE System}
\end{figure}

Layering provides a \textit{hierarchical} structure but provides performance issues as the overhead increases with the addition of each layer. Systems can be modelled as layers, but tend not to be implemented in such a way.

\subsubsection{Hardware Abstraction Layer}

More common in modern OSs. The idea is to have a \textit{core OS}, components such as the file system and scheduler, which don't depend on hardware. Then, you have the \textit{hardware abstraction layer} which specifically deals with the hardware through \textit{drivers} and \textit{assembly routines}.

This allows for more portability, as the layer deals with translation between the core and the specifics of the hardware; developers don't have to worry about specific traits of some hardware and can instead deal with specifics of the core.

\subsubsection{Microkernel}

This concept aims to minimise what the \textit{kernel} is responsible for, and have the rest dealt with by processes at the user level. The Windows Kernel (NT) is a good example.

This isolation allowed isolation between the components, but provides poor performance as the number of user/kernel crossings greatly increases.

\subsubsection{Loadable Kernel Modules}

Utilised by the *NIXes (Linux, BSD, other UNIXes, Solaris), this concept has core services within the kernel code itself, and then other modules can be loaded in \textit{dynamically}.

This style avoids some of the issues that come with layering, as it can call any other module. As the loading is dynamic, there is no need for reboots.



\filbreak
\subsection{Privileged Instructions}

These instructions are restricted to use by the OS \textit{only}, and includes direct access to \textit{I/O devices} and \textit{memory state management}.

This is achieved by the implementation of two \textit{modes of operation}; \textit{user} and \textit{kernel} modes.

Privileged instructions can only be executed in kernel mode.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.35]{privilegeGraphX86}
  \caption{x86 Architecture Levels of Privilege}
  \textit{\footnotesize Courtesy of Hertzsprung of English Wikipedia}
\end{figure}

\subsubsection{System Calls}

If code running from within user mode tries to execute a privileged instruction, it will trigger the \textit{illegal execution trap}, which allows for such code to gain access to privileged instructions and resources. These are called \textit{system calls}.

An OS will define a set of system calls that makes it possible for such an interaction to take place.

\begin{figure}[H]
  \begin{framed}
    \textit{Caller}: The user mode process invoking the system call\\
    \textit{Callee}: The OS handling the system code\\
    \begin{itemize}
    \item The caller places arguments, especially the \textit{type} of system call, in a specified location
    \item The callee saves the caller's state
    \item The callee verifies the arguments
    \item If valid, the code is run
    \item When the system call has been satisfied, the program counter is set to the return address
    \item Execution is returned to user mode
    \end{itemize}
  \end{framed}
  \caption{A rough breakdown of a system call}
\end{figure}

\subsection{Exception Handling}

A \textit{trap} is a synchronised, intended transition that is initiated by the OS. The OS takes control as the result of a system call.

An \textit{exception} is also synchronised, however it is an \textit{unexpected} problem with some instruction.

An \textit{interrupt} is \textit{a}synchronous on the other hand, and is caused by some external device.


\subsection{Processes}

A \textit{process} is a program that is being executed.

A process must have a few key aspects for it to work:

\begin{framed}
  \textbf{Address Space}

  Contains the \textit{instructions} of the running program, and the \textit{data} for running the program.

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.64]{addressspace}
    \caption{An idealised address space given for a process}
  \end{figure}

  \textbf{CPU State}

  Contains the \textit{program counter} which indicates the next instruction, the \textit{stack pointer} and other register values.

  \textbf{OS Resources}

  Contains items that the OS gives the process access to, such as files, network connections and so on.
\end{framed}

A process can spawn another process, which are respectively called the \textit{parent} and \textit{child} of each other. When the parent spawns its child, it can choose to either \textit{wait for the child to terminate}, or \textit{run in parallel with the child}.

Spawning of a child makes use of the \texttt{fork()} process. It returns \texttt{0} to the child, and its process ID to the parent.

\subsection{PCB: Process Control Block}

The operating system needs a way to keep a track of all processes that are running.

A data structure, called the \textit{process control block}, or \textit{process descriptor}, aims to do this.

This name space includes all the \textit{process IDs} of the processes that are currently present. A process ID is a \textit{unique integer}.

A process is called by the \texttt{fork()} function, and can be \textit{killed} (\texttt{kill()}), \textit{paused} (\texttt{wait()}), and \textit{limited} (\texttt{nice()}).

The PCB keeps: the process's \textit{execution state} when the process isn't active; the parent's PID; program counter; and so on. Linux's PCB has over 95 fields for \textit{each} entry.

\begin{figure}[H]
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      \textbf{Ready} & Waiting to be assigned by the CPU\\
      \hline
      \textbf{Running} & The process that is currently running in the CPU\\
      \hline
      \textbf{Waiting} & Cannot progress until some event happens, e.g. \textit{I/O completion}\\
      \hline
    \end{tabular}
    \caption{Possible execution state of a process}
  \end{center}
\end{figure}

When a process is returned to the \textit{running} state, the values of registers are transferred from the PCB to hardware registers. This is called a \textit{context switch}. These occur many, many times per second. The decision of which process to run next is \textit{scheduling}.

\subsection{State Queues}

A set of queues contains a queue for a process within each respective state. These are implemented as a set of \textit{linked lists}. 

\subsection{Sequential Process}

The most simple style of process.

A \textit{sequential process} is given some \textit{address space} and a \textit{thread of execution}. The process will then be executed.

\subsection{Memory Management Unit (MMU)}
Effectively is a hash function from logical address to physical address.

A MMU prevents the need for swapped out processes to be swapped back into the same physical addresses.

Swapping is not typically supported on mobile devices, more likely to to overwrite least used data.


\subsection{Partitioning}
Main memory is usually broken up into two partitions; The OS and user process.

Each process is contained within a single contiguous section on memory.

Reallocation registers are used to protect users processes from one another and from changing the OS code.

Some old techniques include:
\begin{itemize}
    \item Fixed Partitions - simple but causes fragmentation often
    \item Variable Partitions - no internal fragmentation, but can leave holes in the physical memory
\end{itemize}

Dynamic Storage-Allocation is possible using First-fit, Best-fit and Worst-fit in terms of hole filling.


\filbreak
\section{Threading}

Instead of using multiple processes for concurrency, we can use threads, which share the same address
space and OS resources. This smaller overhead usually results in threads being more performant than
the same number of separate processes.

With threads, we want the same program code, data, privileges and resources (open files, sockets...) but
we need multiple execution states (each thread has it's own registers and stack)

From the perspective of the parent process to a set of threads, the \emph{heap} is shared, and each thread has
it's own \emph{stack pointer} and \emph{PC}.

\subsection{Kernel Threading}

The kernel presents it's own threading library, with the kernel managing both the process and threads memory.
Kernel threads are more performant than just processes, but because they are handled through \textbf{system calls}
there's still a sizeable overhead.

\subsection{User-Level Threading}

User threads are built completely transparently to the underlying OS, which only sees the parent processes. User
threads can be very performant, though if one thread is blocked due to IO etc., the entire process will also be
blocked as there is no way for the kernel to schedule other threads as it isn't aware of them.

User threads can be 10 to 100 times faster than kernel threads.

A combination of kernel threading and user-space threading can be used to get the best performance.


\filbreak
\section{Mutual Exclusion \& Locking}

A \emph{critical section} is a sequence of code that may result in
incorrect or undefined behaviour if executed simultaneously or
pre-empted. Similarly, race conditions occur when the order of execution
is unknown and behaviour can be unpredictable.

Mutual Exclusion (MutEx) and locking prevent these problems.

To be safe and operate correctly, a critical section must satisfy the
following requirements:

\begin{description}
\item[Mutual Exclusion]
At most one thread is in the critical section
\item[Progress]
If a thread is outside the critical section, it cannot prevent another
from entering
\item[Bounded Waiting \& No Starvation]
If a thread is waiting to enter the critical section, it is guaranteed
to eventually do so
\item[Performance]
The overhead of entering and exiting the critical section is small
relative to the runtime of the section.
\end{description}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{correctConcurrency}
  \caption{Concurrency Dependencies}
\end{figure}


\subsection{Deadlocks}

We obviously want to avoid deadlocking (and livelocking). For a deadlock to occur, the following four conditions must exist:

\begin{itemize}
\item Mutual Exclusion
\item Hold and Wait (Prevent a process that needs two or more locks from holding one and waiting for the other indefinitely - see the Philosopher's Spaghetti problem)
\item No Preemption i.e. we can't stop a process/thread from running that may be causing a lockup
\item Circular waiting e.g. a process busy waiting on a lock it won't ever get
\end{itemize}

We can see if a deadlock is possible if there is an \emph{irreducible} cycle in the dependency/resource graph for a set of 
processes or threads.

\subsection{Peterson's Algorithm}\label{petersons-algorithm}

\begin{verbatim}
    flag[i] = True;
    turn = 1-i;

    while (flag[1-i] && turn == 1-i); // spin

    /* critical section code */

    flag[i] = False;
\end{verbatim}

Avoids both \emph{deadlock} and \emph{livelock} for two threads, using
\texttt{i} and \texttt{i-1} as signals. It works but can be tricky to
implement.

\subsection{Spinlocks}\label{spinlocks}

A \textbf{spinlock} is a locking primitive, used to build more complex
locking mechanisms. The name comes from the behaviour; It `spins' on a
condition until it is satisfied, so will acquire the lock as soon as it
is available.

\begin{verbatim}
    // do not have lock
    while(some_condition);
    // have lock
\end{verbatim}

Acquiring and releasing locks \emph{must} be an atomic operation, so no
context switching occurs during acquisition which could lead to a crash
or undefined state. \textbf{Test and Set} is an atomic instruction we
can use to achieve this.

\begin{verbatim}
    struct spinlock{
        int held;
    };

    struct spinlock lock;
    lock.held = 0;

    int test_and_set(int* flag){
        int old = *flag;
        flag = 1;
        return old;
    }

    void acquire(struct lock* lock)
    {
        /* This is the `spinning' */
        while(test_and_set(&lock->held));
    }

    void release(struct lock* lock)
    {
        lock->held = 0;
    }
\end{verbatim}

Spinlocks are simple to implement, but can be slow and \emph{block}
whilst waiting for the lock, wasting CPU time.

\subsection{Semaphores}\label{semaphores}

Similar to a spinlock, having the two operations
\texttt{wait(semaphore)} and \texttt{signal(semaphore)}, sometimes given
as \texttt{P(semaphore)} and \texttt{V(semaphore)}. Semaphores can use
integer signals, or a boolean - the boolean semaphore has the same
behaviour as a lock.

\begin{verbatim}
    void wait(semaphore s)
    {
        while(s <= 0); // Busy wait
        s--;
    }

    void signal(semaphore s)
    {
        s++;
    }
\end{verbatim}

This simple implementation uses \emph{busy waiting}. We can use a wait
queue instead, placing ourselves onto the queue and yielding until the
semaphore is available.

\begin{verbatim}
    void wait(semaphore* s)
    {
        s->value--;
        if(s->value < 0){
            enqueue_process();
            /* Add self to the wait queue and block/sleep */
        }
    }

    void signal(semaphore* s)
    {
        s->value++;
        if(s->value <= 0){
            dequeue_process();
            /* Remove a process from the wait queue and wake it */
        }
    }
\end{verbatim}

\subsubsection{Bounded Buffer Problem}\label{bounded-buffer-problem}

In a \textbf{producer and consumer} model, we can have many threads
wishing to consume some data, and many that produce it. The handover of
data can be done with a \emph{bounded buffer}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{produceConsume}
  \caption{Producer-Consumer model}
\end{figure}


We can use three semaphores to ensure safety around this bounded buffer:

\begin{tabular}{l | c | l }
mutex & 1 & Mutual exclusion on shared data (head \& tail\ldots{})\\
empty & n & Number of empty / available slots in the buffer. Initially all available.\\
full  & 0 & Number of taken slots in the buffer. Initially none are taken.
\end{tabular}

\begin{verbatim}
producer:
    wait(empty)
    wait(mutex)
        add item to buffer
    signal(mutex)
    signal(full)

consumer:
    wait(full)
    wait(mutex)
        remove item from buffer
    signal(mutex)
    signal(empty)
\end{verbatim}

\subsubsection{Pub Sub}\label{pub-sub}

The constraints on \textbf{publishers} (aka writers) and
\textbf{subscribers} (aka readers) are different:

\begin{enumerate}
\item
  We \emph{can} allow multiple subscribers to concurrently access
\item
  We \emph{cannot} allow publishers and subscribers to concurrently
  access
\item
  We \emph{cannot} have more than one publisher concurrently writing
\end{enumerate}

This is because subscribers make the promise to not mutate the data,
while publishers do not.

\begin{tabular}{l | c | l }
semaphore: mutex     & 1 & Mutual exclusion on shared data\\
semaphore: writing   & 1 & Lock on mutation/writing to the data\\
integer: read\_count & 0 & The number of subscribers currently reading
\end{tabular}

\begin{verbatim}
writer:
    wait(writing)
        perform writes // Satisfies requirement 3
    signal(writing)
    
reader:
    wait(mutex)
        reader_count++;
        if (reader_count == 1) wait(writing); // Satisfy requirement 2
    signal(mutex)
    
    do reading
    
    wait(mutex)
        reader_count--;
        if (reader_count == 0) signal(writing); // Release lock on writers if no further readers
    signal(mutex)
\end{verbatim}

\subsection{Condition Variables}

Has similar operations to Semaphores: \texttt{wait()} and \texttt{signal()}.

\begin{description}
  \item[Wait] Wait until another thread has signalled and released the lock
  \item[Signal] Wake a thread from the wait queue
\end{description}

Signals aren't remembered if there are no threads in the wait queue like they are with semaphores.

\subsubsection{Bounded Buffer}


\begin{tabular}{l | c | l }
lock:      mutex    & 1 & Mutual exclusion on shared data (head \& tail\ldots{})\\
condition: freeslot & n & There is at least one slot free\\
condition: fullslot & 0 & There is at least one slot taken
\end{tabular}

\begin{verbatim}
producer:
    lock(mutex)
    if [no slots available] wait(freeslot);
        Add item...
    signal(fullslot)
    unlock(mutex)

consumer:
    lock(mutex)
    if [no slots have data] wait(fullslot);
        Pop item from shared buffer
    signal(freeslot)
    unlock(mutex)
    Use item...
\end{verbatim}

\subsection{Possible Bugs}

Most locking mechanisms are prone to bugs; They're shared data structures, and there's never a
guarantee a lock will ever be released or acquired when it should be.

The lock is also not strictly associated with the data it protects; Have we got the right lock? 
Should we have more than one lock? Programming language structures
such as classes with built in locking mechanisms go some way to protect against these errors.


\subsection{Monitors}

A higher level programming language structure, requires some notion of objects (so would be tricky 
in C, much easier in C++). Each method in the class automatically acquires a lock on entry, 
releasing it on exit. This is transparent to the implementer of the API exposed by the class. 
Safeties that come from using classes, such as protected/private methods and structures help protect data.

Implementation of a monitor class in C++. \verb|invariant| is asserted over the life of a class, throwing an
error if the condition is broken. Similarly \verb|precondition| asserts a condition before method
execution is allowed. 

\begin{verbatim}
class Account {
  private lock myLock;

  private int balance := 0
  invariant balance >= 0

  public method boolean withdraw(int amount)
     precondition amount >= 0
  {
    myLock.acquire();
    try:
      if balance < amount then return false
      else { balance := balance - amount ; return true }
    finally:
      myLock.release();
  }

  public method deposit(int amount)
     precondition amount >= 0
  {
    myLock.acquire();
    try:
      balance := balance + amount
    finally:
      myLock.release();
  }
}
\end{verbatim}
\emph{Hoiked from wikipedia, https://en.m.wikipedia.org/wiki/Monitor\_(synchronization)}


\section{Secondary Storage}
%\begin{figure}[H]
%  \centering
%  \includegraphics[scale=0.35]{memoryHierarchy.png}
%  \caption{Memory Hierarchy}
%\end{figure}

\subsection{Disk Performance}
\begin{itemize} 
    \item Seek : moving the disk arm to the correct cylinder
    \item Rotation(latency): waiting for the sector to rotate under the head
    \item Transfer: transferring data from surface to disk controller
\end{itemize}
\vspace{0.5cm}
Seeks are very expensive, so the OS tries to schedule disk requests that are queued waiting for the disk
\begin{itemize}
    \item FCFS - okay when load is low
    \item Shortest Seek Time First (SSTF) - minimizes arm movement, unfairly picks middle blocks
    \item SCAN(elevator algorithm) - service one direction till done, then reverse. Skews wait time non-uniformly
    \item C-SCAN - like SCAN, but only goes in one direction (typewriter)
    \item C-LOOK - similar to C-SCAN, arm only goes as far as final request in each direction
\end{itemize}

\subsection{Paging}

Paging is a \textit{memory management scheme} that an operating system utilises to move information from secondary memory to main memory. A \textit{page} is a fixed-size block that resides in secondary storage.

This scheme is found in many modern operating systems as it allows for programs to exceed the size of physically available memory.

Pages are stored in the \textit{page table} which is stored in main memory. An address is generated by the CPU which states the \textit{page number} which acts as a \textit{base index} of the page table, and offset that provides the physical address when combined with the page number.

With a pure page table, we need \textit{two} accesses to retrieve a page, once to the table to get the location of the page, and then to get the actual contents of the page. We can implement a buffer however to speed this up, the \textit{translation look-aside buffer}, or \textit{associative memory}.

The buffer stores identifiers that uniquely identify each process, but keeps the number of entries quite low, usually with only 64 to 1024 entries.

If a request is made for some page but is no in the buffer, referred to as a \textit{miss}, the value will be loaded in for next time.

\begin{description}
\item[Physical Address] Same as a physical memory address in a simpler page-less system
\item[Logical Address] Is the concatenation of a Page Number and Page Offset, and is the 
addressing scheme exposed to programs.
\item[Page Table] Holds the translations from a \textit{page number} to the base address 
(starting memory address) of a page. Each process has it's own page table.
\item[Page Offset] How far into a page the word of memory we're addressing is.
\item[Page Frame] The divisions of physical memory that can hold a page; A page frame is the same size as a page, but may or may not contain a (valid) page.
\item[PTBR] Page Table Base Register - holds the physical address of the start of the page table
\item[PTLR] Page Table Length Register
\item[TLB] Translation Look-aside Buffer - effectively a fast cache of mappings from Logical to Physical addresses
\end{description}

WORDS.

The size of a page is usually OS specific, sometimes architecture specific. Sizes in the range 4KiB to 8MiB
or more are not unusual; Very small pages, however, are undesirable as the page table itself takes 
memory to keep track of all pages.

Pages can be shared between processes, in a similar way to shared memory amongst threads. The mechanism for this is 
simple, as two processes sharing a page will simply have two different (though not necessarily different) logical 
addresses resolve to the same physical address. Standard concurrency problems can occur. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{tlbPaging}
  \caption{Paging datapath with a TLB}
  \textit{\footnotesize Note that the lookup concurrently runs through the TLB and page table; This is a race but the TLB is significantly faster, so if a hit occurs the Page Table lookup is cancelled.}
\end{figure}

\subsection{Memory Protection Within Pages}

Protection is implemented, to preserve the applicable level of access, such as read-only.

A page also has a \textit{valid/invalid} bit. It states if the page is within the \textit{logical} address space of some process. If \textit{valid}, it is, otherwise it is \textit{invalid}.

\subsection{Benefits of Page Tables}

\begin{itemize}
\item 
  There is memory protection between processes as each address space is unique to the process
\item 
  Common libraries have a shared copy in physical memory instead of one-per-process, which is faster than making a system call
\item 
Different entries per process, so two different processes get different access rights.
\end{itemize}

\subsection{Paging Table Size}

If we were to implement a pure page table, the structure can be very large which would make it rather inefficient.

However, we can use other methods to keep this size down.

\subsubsection{Hierarchical Paging}

Breaks the logical address space into \textit{multiple page tables}.

The page number portion of the address would then be split further, into another page table and offset value.

This quickly gets silly. A page number in a three level paging system for a 64 bit address space is as follows:

\begin{verbatim}

    | 2nd Outer Page | Outer Page | Inner Page | Offset |
    |----------------|------------|------------|--------|
    | 63          32 | 31      22 | 21      12 | 11   0 |

\end{verbatim}

\subsubsection{Hashed Page Table}

Where the page number is hashed into a page table, where each element has the virtual page number,r value of the page frame and a pointer to the next element.

This reduces the amount of entries as each entry in a hashed page table refers to \textit{multiple pages} rather than one page.

\subsubsection{Inverted Page Table}

Rather than each process having a page table, we track all physical pages.

There is one entry in the table for each real page of memory, which contains the virtual address of the page stored in that real location, and information about the process that owns that page.

This reduces the amount of space that is required to store the table but it takes longer to search the table when a page is referenced.





\section{Memory Management}
Programs must be brought from disk into memory and then placed into a process.

The CPU can only access data from memory, not disk.

Register access takes at most 1 CPU cycle, but Main Memory can take many cycles, causing a \emph{Stall}.

\subsection{Base and Limit Registers}
A set of \emph{base} and \emph{limit registers} define the logical address space. The CPU must check every
memory access is valid between the base and the limit for that user. Failure causes a trap in the OS, and usually results in a program being killed (e.g. a \emph{Segmentation Fault})

\subsection{Virtual Address Space}
Logical/Virtual addresses are independent of physical memory.

Hardware translates virtual addresses into physical ones.

Logical/Virtual addresses a process can reference is called the address space.




\section{Virtual Memory}

\subsection{Page Fault}
If there is a reference to a page that isn't stored in memory, then there will be a trap to the OS, a page fault.

The OS then finds a free frame, swaps page into frame via a scheduled disk operation then resets the tables for validity.

Pages are only brought into memory when referenced, this is called demand paging.

\subsection{Page Replacement}
There are a couple ways to pick which page to replace.
\begin{itemize}
    \item Pick a page that won't be needed any time soon
    \item Pick a page that hasn't been used in a while
\end{itemize}

The goal of all of this to reduce the fault rate by selecting the best victim page to remove.
Best being one that will never be touched again.

Belady's proof:
"evicting the page that won’t be used for the longest period of time minimizes page fault rate"

\emph{Thrashing} is when the system spends most of its time serving page faults, and little time doing useful things.

This could mean that there is not enough memory or that the memory is over-committed.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{pageFaultGraph.png}
  \caption{Page Faults vs Number of Frames}
\end{figure}

\subsubsection{Replacement Algorithms}
\begin{itemize}
    \item First-In-First-Out
    \item Belady's Optimal Algorithm - Replace frame that will not be used for the longest time.
        (Not possible as we can't see into the future)
    \item Least Recently Used(LRU)
    \item Replace a page that is 'old enough' - logically, place all pages in a circle and rotate round

\end{itemize}

\vspace{0.5cm}

FIFO and LRU can each be implemented locally or globally.

Local :- Each process has a limited number of frames and operates within them.

Global :- the victim is chosen from all page frames in the system, regardless of owner:




\filbreak
\section{File Systems}

The OS acts as an intermediary between the FS and an Disk. It should hide hardware specifics and allocate disk blocks for the FS.

The file system is responsible for the organisation of files.

For example, Windows' file system NTFS relies on the OS making distinct difference between what disk it is reading from, such as the hard disk or a connected USB thumb drive: \texttt{C:}, \texttt{D:}, etc.

On the other hand, Linux makes no distinction and instead maintains a sole \textit{root}, \texttt{/}, where everything is seen as a 'file', such as a hard disk (\texttt{/dev/sda}), or a media file you may have (\texttt{/home/user/swedgefulBasslines.mp3}). % Fucking cretin % FUCK UP ANGUS YA DICK

A file is a \textit{collection} of data; its \textit{contents}, \textit{size}, \textit{owner}, and so on.

\subsection{Access Methods}

There are a few types of \textit{access methods} that an FS can utilise:

\begin{itemize}
\item
  \textbf{Sequential Access} - read byte by byte
\item
  \textbf{Direct Access} - random access given to block/byte
\item
  \textbf{Record Access} - file is array of records
\item
  \textbf{Indexed Access} - one file contains an index of a record in another file
\end{itemize}

\subsection{Directories}

\textit{Directories}, which can also be referred to as folders, are \textit{nodes} of the file system tree.

A directory contains a unordered list of all files and their attributes within itself. Sorting is usually done by some other utility, such as \texttt{ls}.

One way to find files within these directories, is via \textit{path name translation}.

A file system finds a file when given some \textit{path}, such as \texttt{/home/proper/legit/filepath/ooooo.sh} by walking through the path, directory by directory. At the root, it would first look for \texttt{home}, get its location, then look for \texttt{proper}, get its location, and so on.

\subsection{File Protection}

The file system is responsible for checking of permissions that users have for some given file.

Depending on what the user wants to do to the file, the file system needs to check \textit{which} kind of way it wants to access it, e.g. \textit{read}, \textit{write} or \textit{execute} it. Then, it must check has the rights to perform said action.

This is represented via the \textit{access control matrix}, that lists what users have what access to what files.

\begin{figure}[H]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    & \texttt{/home/hiroshi} & \texttt{/home/boris}\\
    \hline
    Hiroshi & rw & -\\
    \hline
    Boris & r & rw\\
    \hline
    Root & rw & rw\\
    \hline
  \end{tabular}
  \caption{An Access Control Matrix}
\end{figure}

\textit{Access control matrices} are stored as \textit{lists}, which are easier to manage, but grow in size depending on the number of users, or collection of users (\textit{groups}).

\subsection{UNIX inodes}

Stands for index node. An inode contains a file's metadata and the location of it's first blocks on disk.
An \textbf{indirect block} is a block that itself contains pointers to further blocks; With this an inode can represent larger files. Similarly a \textbf{double indirect} block contains pointers to indirect blocks that then poing to data blocks.

So for a filesystem with 512B blocks, and inodes with 12 blocks, 1 indirect and one double indirect block, the maximum filesize an inode can represent is ${12 \times 512B + 512 \times 512B + 512^2 \times 512B = 128.3 MiB}$

\subsection{Organising Blocks}

A file on a disk is stored as a series of \textit{linked blocks}. 

\end{document}
